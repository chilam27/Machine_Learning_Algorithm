{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Input data (\"y\" table) has \"m\" data points, \"n\" columns (features or independent variables), and \"n\" + 1 total of betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the main function, we perform the steps as follow:\n",
    "- First, we initialize the parameters with `initialize_params` based on the dimension of the input data (\"n\")\n",
    "- We then compute the gradient of the betas using `compute_graident`\n",
    "- Use the computed gradients to update the value of each beta using `update_params`\n",
    "- We repeat the process multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "def linear_regression(x, y, iterations = 100, learning_rate = 0.01):\n",
    "    n, m = len(x[0]), len(x)\n",
    "    beta_0, beta_other = initialize_params(n)\n",
    "    for _ in range(iterations):\n",
    "        gradient_beta_0, gradient_beta_other = compute_gradient(x, y, beta_0, beta_other, n, m)\n",
    "        beta_0, beta_other = update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate)\n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `initialize_params` function, we initialize \"beta_0\" as 0 and \"beta_other\" is a vector with the size of \"n\" that holds all the other randomly initialized betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: initialize params\n",
    "def initialize_params(n):\n",
    "    beta_0 = 0\n",
    "    beta_other = [random.random() for _ in range(n)]\n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_gradient` is the core of the algorithm where we compute gradients for all betas.\n",
    "- Initialized all gradient betas as 0\n",
    "- We loop through all data points and add gradient contributed by each data point to those variables:\n",
    "    - First, we obtain the prediction \"y_i_hat\" for each data point \"i\"\n",
    "    - Get the difference between the prediction (\"y_i_hat\") and the observation (\"y[i]\")\n",
    "    - Use the difference to obtain the derivative of the error over \"y\" by multiplying the difference with 2\n",
    "    - Get graident of betas by diving each data point's gradient by \"n\" so the gradient computed at the end will be the average over all data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: compute gradient\n",
    "def compute_gradient(x, y, beta_0, beta_other, n, m):\n",
    "    gradient_beta_0 = 0\n",
    "    gradient_beta_other = [0] * n\n",
    "    \n",
    "    for i in range(m):\n",
    "        y_i_hat = sum(x[i][j] * beta_other[j] for j in range(n)) + beta_0\n",
    "        derror_dy = 2 * (y[i] - y_i_hat)\n",
    "        for j in range(n):\n",
    "            gradient_beta_other[j] += derror_dy * x[i][j] / n\n",
    "        gradient_beta_0 += derror_dy / n\n",
    "    \n",
    "    return gradient_beta_0, gradient_beta_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `update_params` to update all the betas using the gradient we obtained. We don't add gradients to betas, but we scale the gradient by multiplying it with learning rate (a rate of speed where the gradient moves during a gradient descent; learning rate too high will make gradient descent unstable, too low will make it slow to converge)\n",
    "\n",
    "We update betas using \"+=\" because of how gradients are computed in `compute_gradient`. We get the gradient of error with respect to \"y\" as \"y[i]\" - \"y_i_hat\". If \"y_i_hat\" is overestimated, \"derror_dy\" will be a negative value. That's why we add the gradient to betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: update params\n",
    "def update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate):\n",
    "    beta_0 += gradient_beta_0 * learning_rate\n",
    "    for i in range(len(beta_other)):\n",
    "        beta_other[i] += (gradient_beta_other[i] * learning_rate)\n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
