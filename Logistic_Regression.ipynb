{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logsitc Regression From Scratch\n",
    "\n",
    "In this notebook, we will code the logistic regression model from scratch to understand the theory behind this commonly used machine learning model.\n",
    "\n",
    "In the end, we will compare the result of the logistic regression model we build from scratch with the one from scikit-learn.\n",
    "\n",
    "YT: https://youtu.be/gN79XvB7vTo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Input data (\"y\" table) has \"m\" data points, \"n\" columns (features or independent variables), and \"n\" + 1 total of betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the main function, we perform the steps as follows:\n",
    "- Transform \"X\" and \"y\" dataframe into numpy arrays.\n",
    "- Initialize the parameters using `initialize_params` based on the dimension of the input data (\"n\").\n",
    "- Use the next helper function, `compute_gradients`, to derive gradients at each beta.\n",
    "- Use the `update_params` function to update the beta values using the gradients.\n",
    "- Repeat the step for the number of iterations that we've specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "def logistic_regression(X, y, iterations = 100, learning_rate = 0.01):\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    \n",
    "    n, m = len(X[0]), len(X)\n",
    "    beta_0, beta_other = initialize_params(n)\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        gradient_beta_0, gradient_beta_other = compute_gradients(X, y, beta_0, beta_other, n, m)\n",
    "        beta_0, beta_other = update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate)\n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `initialize_params` function, we initialize \"beta_0\" as 0, and \"beta_other\" is a vector with the size of \"n\" that holds all the other randomly initialized betas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: initialize params\n",
    "def initialize_params(n):\n",
    "    beta_0 = 0\n",
    "    beta_other = [random.random() for _ in range(n)]\n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_gradients` is the core of the algorithm where we compute gradients for all betas:\n",
    "- Initialized all gradient betas as 0.\n",
    "- We loop through all data points and add a gradient contributed by each data point to those variables. Inside the `for` loop:\n",
    "    - First, we obtain the prediction \"y_i_hat\" using the `logistic_function`.\n",
    "    - Get the difference (residual) between the prediction (\"y_i_hat\") and the observation (\"y[i]\").\n",
    "    - The gradient for \"beta_0\" is just the residual, but the gradient for \"beta_other\" is the residual multiplied by the feature.\n",
    "    - Accumulate the gradient from all data points (using \"+=\").\n",
    "    - Lastly, divide each data point's gradient by \"m\" so the gradient computed at the end will be the average over all data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: compute gradient\n",
    "def compute_gradients(X, y, beta_0, beta_other, n , m):\n",
    "    gradient_beta_0 = 0\n",
    "    gradient_beta_other = [0] * n\n",
    "    \n",
    "    for i, point in enumerate(X):\n",
    "        y_i_hat = logistic_function(point, beta_0, beta_other)\n",
    "        residual =  y_i_hat - y[i]\n",
    "        for j, feature in enumerate(point):\n",
    "            gradient_beta_other[j] += residual * feature / m\n",
    "        gradient_beta_0 += residual / m\n",
    "            \n",
    "    return gradient_beta_0, gradient_beta_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the logistic function (Sigmoid function) in `logistic_function` to calculate the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: logistic function\n",
    "def logistic_function(point, beta_0, beta_other):\n",
    "    return 1 / (1 + np.exp(-(beta_0 + point.dot(beta_other))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `update_params` to update all the betas using the gradient we obtained. We don't add gradients to betas, but we scale the gradient by multiplying it with the learning rate (a rate of speed where the gradient moves during a gradient descent; a learning rate too high will make gradient descent unstable, too low will make it slow to converge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: update parameters\n",
    "def update_params(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate):\n",
    "    beta_0 -= gradient_beta_0 * learning_rate\n",
    "    for i in range(len(beta_other)):\n",
    "        beta_other[i] -= (gradient_beta_other[i] * learning_rate)\n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "For this section, we will be using the popular Iris dataset to perform the comparison between our logistic regression model and the scikit-learn `LogisticRegression` model. Because our model can only be used for binary classification, we will only predict only 1 species (Iris Setosa) instead of all 3 species that are available in the dataset. The metric we will use for our comparison will be the outputted betas from both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load library\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('data/Iris_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dataset for binary classification\n",
    "df['Species'].replace(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], [1, 0, 0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  Species\n",
       "0       0.222222      0.625000       0.067797      0.041667      1.0\n",
       "1       0.166667      0.416667       0.067797      0.041667      1.0\n",
       "2       0.111111      0.500000       0.050847      0.041667      1.0\n",
       "3       0.083333      0.458333       0.084746      0.041667      1.0\n",
       "4       0.194444      0.666667       0.067797      0.041667      1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize data\n",
    "df_normalized = (df - df.min()) / (df.max() - df.min()) # min-max normalization\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate independent and dependent variables\n",
    "X = df_normalized[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]] # independent variables\n",
    "y = df_normalized[\"Species\"] # dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept | Constant of Logistic Regression Equation:  [1.48912751]\n",
      "Coefficient of Logistic Regression Equation:  [[-1.61033977  1.88360913 -3.31081248 -3.24444535]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn logistic regression model\n",
    "model_sklearn  = LogisticRegression().fit(X, y)\n",
    "\n",
    "print(\"Intercept | Constant of Logistic Regression Equation: \", model_sklearn.intercept_)\n",
    "print(\"Coefficient of Logistic Regression Equation: \", model_sklearn.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept | Constant of Logistic Regression Equation:  1.260213131716376\n",
      "Coefficient of Logistic Regression Equation:  [-1.6857031217591012, 2.726790512780263, -3.5746320923493586, -3.687798116572437]\n"
     ]
    }
   ],
   "source": [
    "# our logistic regression model\n",
    "model_scratch = logistic_regression(X, y, iterations = 1000, learning_rate = 0.1)\n",
    "\n",
    "print(\"Intercept | Constant of Logistic Regression Equation: \", model_scratch[0])\n",
    "print(\"Coefficient of Logistic Regression Equation: \", model_scratch[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the results above, both the intercepts (beta_0) and the coefficients (beta_other) are very similar between the two models. Hence, we have successfully built a logistic regression from scratch!\n",
    "\n",
    "One thing to note is our model is very sensitive to any characteristic the dataset might have. For example, for us to get the betas from our scratch model to be as similar to the scikit-learn's model, we had to perform (min-max) normalization on the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlalgorithm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
